{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/wlsgur4011/SparseDC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "EXP = \"final_version \"\n",
    "NAME = \"final_version\"\n",
    "CKPT_PATH = \"pretrain/nyu.ckpt\"\n",
    "\n",
    "sys.argv = [\"eval.py\", \n",
    "           f\"experiment={EXP}\", \n",
    "           f\"num_sample=shift_grid\",\n",
    "           f\"task_name={NAME}_shift_grid_sample\", \n",
    "           f\"ckpt_path={CKPT_PATH}\", \n",
    "           \"++trainer.deterministic=True\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HYDRA_FULL_ERROR=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import Logger\n",
    "\n",
    "from src import utils\n",
    "\n",
    "log = utils.get_pylogger(__name__)\n",
    "\n",
    "def evaluate(cfg: DictConfig) -> Tuple[dict, dict]:\n",
    "    \"\"\"Evaluates given checkpoint on a datamodule testset.\n",
    "\n",
    "    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n",
    "    before and after the call.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): Configuration composed by Hydra.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n",
    "    \"\"\"\n",
    "\n",
    "    assert cfg.ckpt_path\n",
    "    if cfg.get(\"seed\"):\n",
    "        pl.seed_everything(cfg.seed)\n",
    "\n",
    "    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n",
    "    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n",
    "\n",
    "    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n",
    "    model: LightningModule = hydra.utils.instantiate(cfg.model)\n",
    "\n",
    "    log.info(\"Instantiating loggers...\")\n",
    "    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n",
    "\n",
    "    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n",
    "    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)\n",
    "\n",
    "    object_dict = {\n",
    "        \"cfg\": cfg,\n",
    "        \"datamodule\": datamodule,\n",
    "        \"model\": model,\n",
    "        \"logger\": logger,\n",
    "        \"trainer\": trainer,\n",
    "    }\n",
    "\n",
    "    if logger:\n",
    "        log.info(\"Logging hyperparameters!\")\n",
    "        utils.log_hyperparameters(object_dict)\n",
    "\n",
    "    log.info(\"Starting testing!\")\n",
    "\n",
    "    if datamodule.hparams.dataset in [\"nyu\", \"sunrgbd\"]:\n",
    "        results = {}\n",
    "        if datamodule.hparams.dataset == \"nyu\":\n",
    "            # iter = 10\n",
    "            iter = 1\n",
    "        else:\n",
    "            iter = 1\n",
    "        for i in range(iter):\n",
    "            pl.seed_everything(cfg.seed + i)\n",
    "            trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)\n",
    "            metric_dict = trainer.callback_metrics\n",
    "            for k in metric_dict.keys():\n",
    "                if k[5:] in results:\n",
    "                    results[k[5:]].append(metric_dict[k].item())\n",
    "                else:\n",
    "                    results[k[5:]] = [metric_dict[k].item()]\n",
    "        import csv\n",
    "        import numpy as np\n",
    "\n",
    "        with open(model.val_csv, \"a\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=model.fieldnames)\n",
    "            output = {}\n",
    "            output[\"epoch\"] = \"Result\"\n",
    "            for k in results.keys():\n",
    "                output[k] = f\"{np.mean(results[k]):.4f}Â±{np.std(results[k]):.4f}\"\n",
    "                print(f\"{k:5s}: {output[k]}\")\n",
    "            writer.writerow(output)\n",
    "    else:\n",
    "        # trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)\n",
    "        trainer.validate(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)  # for kitti test\n",
    "        metric_dict = trainer.callback_metrics\n",
    "\n",
    "    return metric_dict, object_dict\n",
    "\n",
    "\n",
    "@hydra.main(version_base=\"1.3\", config_path=\"configs\", config_name=\"eval.yaml\")\n",
    "def main(cfg: DictConfig) -> None:\n",
    "    # import jhutil; jhutil.jhprint(1111, cfg)\n",
    "    evaluate(cfg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsedc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
